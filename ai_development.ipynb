{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Detection AI Development Notebook\n",
    "\n",
    "This notebook provides an interactive environment for developing and testing the wildfire detection AI model.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Dataset Exploration](#dataset)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Model Development](#model)\n",
    "5. [Training](#training)\n",
    "6. [Evaluation](#evaluation)\n",
    "7. [TensorFlow Lite Conversion](#conversion)\n",
    "8. [Performance Analysis](#performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if running in Colab\n",
    "# !pip install tensorflow matplotlib seaborn scikit-learn pillow opencv-python\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Exploration <a name=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_PATH = \"dataset\"\n",
    "FIRE_DATASET_PATH = os.path.join(DATASET_PATH, \"fire\")\n",
    "NO_FIRE_DATASET_PATH = os.path.join(DATASET_PATH, \"no_fire\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FIRE_DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(NO_FIRE_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "def analyze_dataset():\n",
    "    \"\"\"Analyze dataset characteristics\"\"\"\n",
    "    \n",
    "    # Get image paths\n",
    "    fire_images = glob.glob(os.path.join(FIRE_DATASET_PATH, \"*.jpg\")) + \\\n",
    "                 glob.glob(os.path.join(FIRE_DATASET_PATH, \"*.png\"))\n",
    "    \n",
    "    no_fire_images = glob.glob(os.path.join(NO_FIRE_DATASET_PATH, \"*.jpg\")) + \\\n",
    "                    glob.glob(os.path.join(NO_FIRE_DATASET_PATH, \"*.png\"))\n",
    "    \n",
    "    print(f\"Fire images: {len(fire_images)}\")\n",
    "    print(f\"No-fire images: {len(no_fire_images)}\")\n",
    "    print(f\"Total images: {len(fire_images) + len(no_fire_images)}\")\n",
    "    \n",
    "    # Analyze image sizes\n",
    "    fire_sizes = []\n",
    "    no_fire_sizes = []\n",
    "    \n",
    "    for img_path in fire_images[:50]:  # Sample first 50\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            fire_sizes.append(img.size)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for img_path in no_fire_images[:50]:  # Sample first 50\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            no_fire_sizes.append(img.size)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Plot size distributions\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    if fire_sizes:\n",
    "        fire_widths = [s[0] for s in fire_sizes]\n",
    "        fire_heights = [s[1] for s in fire_sizes]\n",
    "        plt.scatter(fire_widths, fire_heights, alpha=0.6, label='Fire')\n",
    "    if no_fire_sizes:\n",
    "        no_fire_widths = [s[0] for s in no_fire_sizes]\n",
    "        no_fire_heights = [s[1] for s in no_fire_sizes]\n",
    "        plt.scatter(no_fire_widths, no_fire_heights, alpha=0.6, label='No Fire')\n",
    "    plt.xlabel('Width')\n",
    "    plt.ylabel('Height')\n",
    "    plt.title('Image Size Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    labels = ['Fire', 'No Fire']\n",
    "    sizes = [len(fire_images), len(no_fire_images)]\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Class Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fire_images, no_fire_images\n",
    "\n",
    "# Analyze dataset\n",
    "fire_images, no_fire_images = analyze_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "def show_sample_images(image_paths, title, num_samples=5):\n",
    "    \"\"\"Display sample images from a class\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    \n",
    "    for i in range(min(num_samples, len(image_paths))):\n",
    "        plt.subplot(1, num_samples, i+1)\n",
    "        img = Image.open(image_paths[i])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Sample {i+1}\\n{img.size}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show sample images from each class\n",
    "if fire_images:\n",
    "    show_sample_images(fire_images, \"Fire Images\")\n",
    "\n",
    "if no_fire_images:\n",
    "    show_sample_images(no_fire_images, \"No-Fire Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing <a name=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_HEIGHT = 96\n",
    "IMG_WIDTH = 96\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "def create_data_generators():\n",
    "    \"\"\"Create training and validation data generators\"\"\"\n",
    "    \n",
    "    # Training data generator with augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        validation_split=VALIDATION_SPLIT\n",
    "    )\n",
    "    \n",
    "    # Validation data generator (no augmentation)\n",
    "    val_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=VALIDATION_SPLIT\n",
    "    )\n",
    "    \n",
    "    # Training generator\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        DATASET_PATH,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Validation generator\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        DATASET_PATH,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Class indices: {train_generator.class_indices}\")\n",
    "    \n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Create data generators\n",
    "train_generator, validation_generator = create_data_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data augmentation\n",
    "def visualize_augmentation(generator, num_images=5):\n",
    "    \"\"\"Visualize data augmentation effects\"\"\"\n",
    "    \n",
    "    # Get a batch of images\n",
    "    images, labels = next(generator)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.suptitle('Data Augmentation Examples', fontsize=16)\n",
    "    \n",
    "    for i in range(min(num_images, len(images))):\n",
    "        plt.subplot(2, num_images//2 + 1, i+1)\n",
    "        plt.imshow(images[i])\n",
    "        class_name = 'Fire' if np.argmax(labels[i]) == 0 else 'No Fire'\n",
    "        plt.title(f'{class_name}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize augmented images\n",
    "visualize_augmentation(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    \"\"\"Create a custom CNN model optimized for TinyML\"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        \n",
    "        # First convolutional block\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(2, activation='softmax')  # Fire, No-Fire\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_mobilenet_model():\n",
    "    \"\"\"Create MobileNetV2-based model\"\"\"\n",
    "    \n",
    "    # Load MobileNetV2 without top layers\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "MODEL_TYPE = 'cnn'  # or 'mobilenet'\n",
    "\n",
    "if MODEL_TYPE == 'cnn':\n",
    "    model = create_cnn_model()\n",
    "else:\n",
    "    model = create_mobilenet_model()\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training <a name=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training callbacks\n",
    "def get_callbacks():\n",
    "    \"\"\"Get training callbacks\"\"\"\n",
    "    \n",
    "    callbacks = []\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stopping)\n",
    "    \n",
    "    # Model checkpoint\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        'models/best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(model_checkpoint)\n",
    "    \n",
    "    # Learning rate reduction\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(reduce_lr)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "# Get callbacks\n",
    "callbacks = get_callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 50\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation <a name=\"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(history.history['loss'], label='Training')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision\n",
    "    if 'precision' in history.history:\n",
    "        ax3.plot(history.history['precision'], label='Training')\n",
    "        ax3.plot(history.history['val_precision'], label='Validation')\n",
    "        ax3.set_title('Model Precision')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Precision')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recall\n",
    "    if 'recall' in history.history:\n",
    "        ax4.plot(history.history['recall'], label='Training')\n",
    "        ax4.plot(history.history['val_recall'], label='Validation')\n",
    "        ax4.set_title('Model Recall')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Recall')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "def evaluate_model(model, validation_generator):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(validation_generator)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = validation_generator.classes\n",
    "    \n",
    "    # Classification report\n",
    "    class_names = list(validation_generator.class_indices.keys())\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'y_pred': y_pred,\n",
    "        'y_true': y_true,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "\n",
    "# Evaluate model\n",
    "evaluation_results = evaluate_model(model, validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TensorFlow Lite Conversion <a name=\"conversion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Lite\n",
    "def convert_to_tflite(model, quantization='dynamic'):\n",
    "    \"\"\"Convert Keras model to TensorFlow Lite\"\"\"\n",
    "    \n",
    "    print(f\"Converting model to TensorFlow Lite ({quantization} quantization)...\")\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if quantization == 'dynamic':\n",
    "        # Dynamic range quantization\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    elif quantization == 'full_int8':\n",
    "        # Full integer quantization\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
    "        ]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "        \n",
    "        # Representative dataset for quantization\n",
    "        def representative_dataset_gen():\n",
    "            # Load a few sample images for quantization calibration\n",
    "            for img_path in glob.glob(os.path.join(DATASET_PATH, \"**/*.jpg\"))[:100]:\n",
    "                img = tf.io.read_file(img_path)\n",
    "                img = tf.image.decode_jpeg(img, channels=3)\n",
    "                img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "                img = tf.cast(img, tf.float32) / 255.0\n",
    "                img = tf.expand_dims(img, 0)\n",
    "                yield [img]\n",
    "        \n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "    elif quantization == 'float16':\n",
    "        # Float16 quantization\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    # Convert model\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save model\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    tflite_path = f'models/fire_detection_{quantization}.tflite'\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    # Get model size\n",
    "    model_size = len(tflite_model)\n",
    "    print(f\"TensorFlow Lite model saved: {tflite_path}\")\n",
    "    print(f\"Model size: {model_size} bytes ({model_size/1024:.1f} KB)\")\n",
    "    \n",
    "    return tflite_model, model_size\n",
    "\n",
    "# Convert model to TFLite\n",
    "QUANTIZATION_TYPE = 'dynamic'  # 'dynamic', 'full_int8', or 'float16'\n",
    "tflite_model, model_size = convert_to_tflite(model, QUANTIZATION_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to C array for ESP32\n",
    "def convert_to_c_array(tflite_model, array_name='model_data'):\n",
    "    \"\"\"Convert TFLite model to C array for embedding\"\"\"\n",
    "    \n",
    "    print(\"Converting to C array...\")\n",
    "    \n",
    "    # Convert to hex array\n",
    "    hex_array = [f'0x{byte:02x}' for byte in tflite_model]\n",
    "    \n",
    "    # Create C file content\n",
    "    c_content = f'''// Auto-generated TensorFlow Lite model data\n",
    "// Generated on {pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "// Model size: {len(tflite_model)} bytes\\n\n",
    "#ifndef MODEL_DATA_H_\\n\n",
    "#define MODEL_DATA_H_\\n\n",
    "\\n\n",
    "#include <cstdint>\\n\n",
    "\\n\n",
    "const unsigned int {array_name}_len = {len(tflite_model)};\\n\n",
    "const unsigned char {array_name}[] = {{\\n\n",
    "    {', '.join(hex_array[:16])}'''\n",
    "    \n",
    "    # Add remaining bytes in chunks\n",
    "    for i in range(16, len(hex_array), 16):\n",
    "        chunk = ', '.join(hex_array[i:i+16])\n",
    "        c_content += f''',\\n    {chunk}'''\n",
    "    \n",
    "    c_content += f'''\\n}};\\n\n",
    "\\n\n",
    "#endif  // MODEL_DATA_H_\\n\n",
    "'''\n",
    "    \n",
    "    # Save C file\n",
    "    c_file_path = f'models/model_data_{QUANTIZATION_TYPE}.h'\n",
    "    with open(c_file_path, 'w') as f:\n",
    "        f.write(c_content)\n",
    "    \n",
    "    print(f\"C array saved to: {c_file_path}\")\n",
    "    return c_file_path\n",
    "\n",
    "# Convert to C array\n",
    "c_file_path = convert_to_c_array(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis <a name=\"performance\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference speed\n",
    "def test_inference_speed(model, test_images=100):\n",
    "    \"\"\"Test model inference speed\"\"\"\n",
    "    \n",
    "    print(f\"Testing inference speed on {test_images} images...\")\n",
    "    \n",
    "    # Get sample images\n",
    "    image_files = []\n",
    "    for class_dir in [FIRE_DATASET_PATH, NO_FIRE_DATASET_PATH]:\n",
    "        image_files.extend(glob.glob(os.path.join(class_dir, \"*.jpg\"))[:test_images//2])\n",
    "    \n",
    "    inference_times = []\n",
    "    \n",
    "    for img_path in image_files[:test_images]:\n",
    "        # Load and preprocess image\n",
    "        img = keras.preprocessing.image.load_img(\n",
    "            img_path, target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
    "        )\n",
    "        img_array = keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0)\n",
    "        img_array = img_array / 255.0\n",
    "        \n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        inference_times.append(inference_time)\n",
    "    \n",
    "    avg_time = np.mean(inference_times)\n",
    "    min_time = np.min(inference_times)\n",
    "    max_time = np.max(inference_times)\n",
    "    \n",
    "    print(f\"Average inference time: {avg_time:.2f} ms\")\n",
    "    print(f\"Min inference time: {min_time:.2f} ms\")\n",
    "    print(f\"Max inference time: {max_time:.2f} ms\")\n",
    "    \n",
    "    # Plot inference time distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(inference_times, bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(avg_time, color='red', linestyle='--', label=f'Average: {avg_time:.1f}ms')\n",
    "    plt.xlabel('Inference Time (ms)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Inference Time Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'avg_inference_time': avg_time,\n",
    "        'min_inference_time': min_time,\n",
    "        'max_inference_time': max_time,\n",
    "        'inference_times': inference_times\n",
    "    }\n",
    "\n",
    "# Test inference speed\n",
    "speed_results = test_inference_speed(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model complexity\n",
    "def analyze_model_complexity(model):\n",
    "    \"\"\"Analyze model complexity and parameters\"\"\"\n",
    "    \n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([layer.count_params() for layer in model.layers if layer.trainable])\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    \n",
    "    print(\"Model Complexity Analysis:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "    print(f\"Model size (approx): {total_params * 4 / 1024:.1f} KB (32-bit floats)\")\n",
    "    \n",
    "    # Layer-wise analysis\n",
    "    print(\"\\nLayer-wise parameter count:\")\n",
    "    for layer in model.layers:\n",
    "        params = layer.count_params()\n",
    "        if params > 0:\n",
    "            print(f\"  {layer.name}: {params:,} parameters\")\n",
    "    \n",
    "    return {\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'non_trainable_params': non_trainable_params\n",
    "    }\n",
    "\n",
    "# Analyze model complexity\n",
    "complexity_analysis = analyze_model_complexity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "def save_training_results(history, evaluation_results, speed_results, complexity_analysis, model_size):\n",
    "    \"\"\"Save training results to file\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'model_config': {\n",
    "            'type': MODEL_TYPE,\n",
    "            'input_shape': [IMG_HEIGHT, IMG_WIDTH, 3],\n",
    "            'quantization': QUANTIZATION_TYPE,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'epochs': len(history.history['accuracy'])\n",
    "        },\n",
    "        'final_metrics': {\n",
    "            'accuracy': history.history['accuracy'][-1],\n",
    "            'val_accuracy': history.history['val_accuracy'][-1],\n",
    "            'loss': history.history['loss'][-1],\n",
    "            'val_loss': history.history['val_loss'][-1]\n",
    "        },\n",
    "        'inference_performance': speed_results,\n",
    "        'model_complexity': complexity_analysis,\n",
    "        'model_size_kb': model_size / 1024,\n",
    "        'classification_report': evaluation_results['classification_report']\n",
    "    }\n",
    "    \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    results_path = f'models/training_results_{QUANTIZATION_TYPE}.json'\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        import json\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Training results saved to: {results_path}\")\n",
    "    \n",
    "    # Also save model in SavedModel format\n",
    "    model.save(f'models/saved_model_{QUANTIZATION_TYPE}')\n",
    "    print(f\"Model saved in SavedModel format\")\n",
    "    \n",
    "    return results_path\n",
    "\n",
    "# Save results\n",
    "results_path = save_training_results(\n",
    "    history, evaluation_results, speed_results, \n",
    "    complexity_analysis, model_size\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AI Development Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_TYPE.upper()} ({QUANTIZATION_TYPE} quantization)\")\n",
    "print(f\"Final Accuracy: {history.history['val_accuracy'][-1]:.1f}%\")\n",
    "print(f\"Average Inference Time: {speed_results['avg_inference_time']:.1f}ms\")\n",
    "print(f\"Model Size: {model_size/1024:.1f}KB\")\n",
    "print(f\"Files saved in: models/ directory\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/plain",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
